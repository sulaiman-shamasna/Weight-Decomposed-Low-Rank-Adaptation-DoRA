{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decomposed Low-Rank Adaptation (DoRA)\n",
    "---\n",
    "Low-rank adaptation(**LoRA**) is a machine learning technique that modifies a pretrained model (e.g., an LLM) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters.\n",
    "\n",
    "This approach is important because it allows for efficient finetuning of large models on task-specific data significantly reducing the computational cost and time required for finetuning.\n",
    "\n",
    "In this notebook, we are going to talk about [Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353), which is a new alterative to **LoRA**, which may outperform LoRA by a large margin. We are going to implement both **LoRA** and **DoRA** in PyTorch from scratch in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DoRA** can be seen as an improvement or extension of **LoRA** that is built on top of it, and we can now easily adapt some of our previous code to implement **DoRA**. **DoRA** can be described in two steps, where the first step is to decompose a pretrained weight matrix into a magnitude *vector(m)* and a directional *matrix(V)*. The second step is applyting **LoRA** to the directional matrix *V* and training the magnitude vector *m* separately.\n",
    "The decomposition into magnitude and directional components is inspired by the mathematical principle that any vector can be represented as the product of its magnitude(a scalar value indicating its length) and its direction (a unit vector indicating its orientation in space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up work environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from cuda import DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    DEVICE=torch.device('cuda')\n",
    "else:\n",
    "    DEVICE=torch.device('cpu')\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement **LoRA** and **DoRA** Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev=1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A=nn.Parameter(torch.randn(in_dim, rank)*std_dev)\n",
    "        self.B=nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha=alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # @ means matrix multiplication\n",
    "        x=self.alpha*(x @ self.A @ self.B)\n",
    "        return x\n",
    "    \n",
    "class LinearWithDoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        self.m=nn.Parameter(torch.ones(1, linear.out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_output=self.linear(x)\n",
    "        lora_output=self.lora(x)\n",
    "        lora_output_norm=lora_output/lora_output.norm(p=2, dim=1, keepdim=True)\n",
    "        dora_modification=self.m * lora_output_norm\n",
    "        dora_output=self.lora(x)\n",
    "        return linear_output+dora_output\n",
    "    \n",
    "# this code is equivalent to LinearWithDoRA\n",
    "class LinearWithDoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        self.m=nn.Parameter(self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lora=self.lora.A @self.lora.B\n",
    "        numerator=self.linear.weight+self.lora.alpha*lora.T\n",
    "        denominator=numerator.norm(p=2, dim=0, keepdim=True)\n",
    "        directional_component=numerator/denominator\n",
    "        new_weight=self.m*directional_component\n",
    "        return F.linear(x, new_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Swaping existing Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>) \n",
      " ------------\n",
      "tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "random_seed=123\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "layer=nn.Linear(10,2)\n",
    "x=torch.randn(1,10)\n",
    "\n",
    "layer_dora_1=LinearWithDoRA(layer, rank=2, alpha=4)\n",
    "print(layer_dora_1(x), '\\n ------------')\n",
    "layer_dora_2=LinearWithDoRAMerged(layer, rank=2, alpha=4)\n",
    "print(layer_dora_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate=0.005\n",
    "num_epochs=15\n",
    "\n",
    "# architecture\n",
    "num_features=784\n",
    "num_hidden_1=128\n",
    "num_hidden_2=256\n",
    "num_classes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Multilayer Perceptron Model (Without **DoRA**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "\n",
    "model=MultilayerPerceptron(\n",
    "    num_features=num_features,\n",
    "    num_hidden_1=num_hidden_1,\n",
    "    num_hidden_2=num_hidden_2,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "optimizer_pretrained=torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# note transforms.ToTensor() scales input images to 0-1 range\n",
    "train_dataset=datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset=datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader=DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader=DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# checking the dataset\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Evaluation and Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/015 | Batch 000/938 | Loss: 2.3110\n",
      "Epoch 001/015 | Batch 400/938 | Loss: 0.2177\n",
      "Epoch 001/015 | Batch 800/938 | Loss: 0.3389\n",
      "Epoch: 001/015 training accuracy: 95.14%\n",
      "Time elapsed: 0.09 min\n",
      "Epoch 002/015 | Batch 000/938 | Loss: 0.1139\n",
      "Epoch 002/015 | Batch 400/938 | Loss: 0.1833\n",
      "Epoch 002/015 | Batch 800/938 | Loss: 0.2406\n",
      "Epoch: 002/015 training accuracy: 96.94%\n",
      "Time elapsed: 0.18 min\n",
      "Epoch 003/015 | Batch 000/938 | Loss: 0.0661\n",
      "Epoch 003/015 | Batch 400/938 | Loss: 0.0905\n",
      "Epoch 003/015 | Batch 800/938 | Loss: 0.2603\n",
      "Epoch: 003/015 training accuracy: 97.14%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch 004/015 | Batch 000/938 | Loss: 0.0786\n",
      "Epoch 004/015 | Batch 400/938 | Loss: 0.1385\n",
      "Epoch 004/015 | Batch 800/938 | Loss: 0.3343\n",
      "Epoch: 004/015 training accuracy: 97.71%\n",
      "Time elapsed: 0.34 min\n",
      "Epoch 005/015 | Batch 000/938 | Loss: 0.0386\n",
      "Epoch 005/015 | Batch 400/938 | Loss: 0.1358\n",
      "Epoch 005/015 | Batch 800/938 | Loss: 0.0768\n",
      "Epoch: 005/015 training accuracy: 96.85%\n",
      "Time elapsed: 0.43 min\n",
      "Epoch 006/015 | Batch 000/938 | Loss: 0.0259\n",
      "Epoch 006/015 | Batch 400/938 | Loss: 0.0662\n",
      "Epoch 006/015 | Batch 800/938 | Loss: 0.1387\n",
      "Epoch: 006/015 training accuracy: 98.06%\n",
      "Time elapsed: 0.53 min\n",
      "Epoch 007/015 | Batch 000/938 | Loss: 0.1018\n",
      "Epoch 007/015 | Batch 400/938 | Loss: 0.2426\n",
      "Epoch 007/015 | Batch 800/938 | Loss: 0.1290\n",
      "Epoch: 007/015 training accuracy: 98.06%\n",
      "Time elapsed: 0.62 min\n",
      "Epoch 008/015 | Batch 000/938 | Loss: 0.0586\n",
      "Epoch 008/015 | Batch 400/938 | Loss: 0.0042\n",
      "Epoch 008/015 | Batch 800/938 | Loss: 0.2845\n",
      "Epoch: 008/015 training accuracy: 98.22%\n",
      "Time elapsed: 0.71 min\n",
      "Epoch 009/015 | Batch 000/938 | Loss: 0.1367\n",
      "Epoch 009/015 | Batch 400/938 | Loss: 0.0213\n",
      "Epoch 009/015 | Batch 800/938 | Loss: 0.1054\n",
      "Epoch: 009/015 training accuracy: 97.65%\n",
      "Time elapsed: 0.80 min\n",
      "Epoch 010/015 | Batch 000/938 | Loss: 0.0298\n",
      "Epoch 010/015 | Batch 400/938 | Loss: 0.0216\n",
      "Epoch 010/015 | Batch 800/938 | Loss: 0.1946\n",
      "Epoch: 010/015 training accuracy: 98.41%\n",
      "Time elapsed: 0.89 min\n",
      "Epoch 011/015 | Batch 000/938 | Loss: 0.0205\n",
      "Epoch 011/015 | Batch 400/938 | Loss: 0.0759\n",
      "Epoch 011/015 | Batch 800/938 | Loss: 0.1666\n",
      "Epoch: 011/015 training accuracy: 98.65%\n",
      "Time elapsed: 0.97 min\n",
      "Epoch 012/015 | Batch 000/938 | Loss: 0.0001\n",
      "Epoch 012/015 | Batch 400/938 | Loss: 0.0871\n",
      "Epoch 012/015 | Batch 800/938 | Loss: 0.0253\n",
      "Epoch: 012/015 training accuracy: 98.28%\n",
      "Time elapsed: 1.06 min\n",
      "Epoch 013/015 | Batch 000/938 | Loss: 0.0188\n",
      "Epoch 013/015 | Batch 400/938 | Loss: 0.0469\n",
      "Epoch 013/015 | Batch 800/938 | Loss: 0.0742\n",
      "Epoch: 013/015 training accuracy: 98.70%\n",
      "Time elapsed: 1.15 min\n",
      "Epoch 014/015 | Batch 000/938 | Loss: 0.0012\n",
      "Epoch 014/015 | Batch 400/938 | Loss: 0.0016\n",
      "Epoch 014/015 | Batch 800/938 | Loss: 0.3276\n",
      "Epoch: 014/015 training accuracy: 98.93%\n",
      "Time elapsed: 1.23 min\n",
      "Epoch 015/015 | Batch 000/938 | Loss: 0.0012\n",
      "Epoch 015/015 | Batch 400/938 | Loss: 0.0004\n",
      "Epoch 015/015 | Batch 800/938 | Loss: 0.0663\n",
      "Epoch: 015/015 training accuracy: 98.79%\n",
      "Time elapsed: 1.32 min\n",
      "Total Training TIme: 1.32 min\n",
      "Test accuracy: 97.03%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples=0,0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            logits=model(features)\n",
    "            _, predicted_labels=torch.max(logits,1)\n",
    "            num_examples+=targets.size(0)\n",
    "            correct_pred+=(predicted_labels==targets).sum()\n",
    "        return correct_pred.float()/num_examples*100\n",
    "\n",
    "def train(num_epochs, model, optimizer, train_loader, device):\n",
    "    start_time=time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            \n",
    "            # Forward and backpropgation\n",
    "            logits=model(features)\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            #logging\n",
    "            if not batch_idx %400:\n",
    "                print('Epoch %03d/%03d | Batch %03d/%03d | Loss: %.4f' % (epoch+1, num_epochs, batch_idx, len(train_loader), loss))\n",
    "        with torch.set_grad_enabled(False):\n",
    "            print('Epoch: %03d/%03d training accuracy: %0.2f%%' % (epoch+1, num_epochs, compute_accuracy(model, train_loader, device)))\n",
    "        print('Time elapsed: %.2f min' % ((time.time()- start_time)/60))\n",
    "    print('Total Training TIme: %.2f min' % ((time.time()-start_time)/60))\n",
    "    \n",
    "\n",
    "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      " -----------\n",
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithDoRAMerged(\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithDoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithDoRAMerged(\n",
      "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ") \n",
      " -----------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model_dora=copy.deepcopy(model)\n",
    "print(model_dora, '\\n -----------')\n",
    "\n",
    "model_dora.layers[0]=LinearWithDoRAMerged(model_dora.layers[0], rank=4, alpha=8)\n",
    "model_dora.layers[2]=LinearWithDoRAMerged(model_dora.layers[2], rank=4, alpha=8)\n",
    "model_dora.layers[4]=LinearWithDoRAMerged(model_dora.layers[4], rank=4, alpha=8)\n",
    "\n",
    "model_dora.to(DEVICE)\n",
    "optimizer_dora=torch.optim.Adam(model_dora.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model_dora, '\\n -----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Freeze the orignal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.m: True\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.m: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.m: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_dora)\n",
    "\n",
    "# check if linear layers are frozen\n",
    "for name, param in model_dora.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')\n",
    "\n",
    "print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/015 | Batch 000/938 | Loss: 0.0001\n",
      "Epoch 001/015 | Batch 400/938 | Loss: 0.0127\n",
      "Epoch 001/015 | Batch 800/938 | Loss: 0.0228\n",
      "Epoch: 001/015 training accuracy: 98.93%\n",
      "Time elapsed: 0.13 min\n",
      "Epoch 002/015 | Batch 000/938 | Loss: 0.0343\n",
      "Epoch 002/015 | Batch 400/938 | Loss: 0.0036\n",
      "Epoch 002/015 | Batch 800/938 | Loss: 0.0421\n",
      "Epoch: 002/015 training accuracy: 98.96%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch 003/015 | Batch 000/938 | Loss: 0.0086\n",
      "Epoch 003/015 | Batch 400/938 | Loss: 0.0700\n",
      "Epoch 003/015 | Batch 800/938 | Loss: 0.0146\n",
      "Epoch: 003/015 training accuracy: 98.93%\n",
      "Time elapsed: 0.38 min\n",
      "Epoch 004/015 | Batch 000/938 | Loss: 0.0319\n",
      "Epoch 004/015 | Batch 400/938 | Loss: 0.0062\n",
      "Epoch 004/015 | Batch 800/938 | Loss: 0.0480\n",
      "Epoch: 004/015 training accuracy: 99.23%\n",
      "Time elapsed: 0.51 min\n",
      "Epoch 005/015 | Batch 000/938 | Loss: 0.0155\n",
      "Epoch 005/015 | Batch 400/938 | Loss: 0.0190\n",
      "Epoch 005/015 | Batch 800/938 | Loss: 0.1159\n",
      "Epoch: 005/015 training accuracy: 99.06%\n",
      "Time elapsed: 0.63 min\n",
      "Epoch 006/015 | Batch 000/938 | Loss: 0.0738\n",
      "Epoch 006/015 | Batch 400/938 | Loss: 0.0211\n",
      "Epoch 006/015 | Batch 800/938 | Loss: 0.0230\n",
      "Epoch: 006/015 training accuracy: 99.14%\n",
      "Time elapsed: 0.76 min\n",
      "Epoch 007/015 | Batch 000/938 | Loss: 0.0045\n",
      "Epoch 007/015 | Batch 400/938 | Loss: 0.0023\n",
      "Epoch 007/015 | Batch 800/938 | Loss: 0.0043\n",
      "Epoch: 007/015 training accuracy: 99.25%\n",
      "Time elapsed: 0.88 min\n",
      "Epoch 008/015 | Batch 000/938 | Loss: 0.0128\n",
      "Epoch 008/015 | Batch 400/938 | Loss: 0.1230\n",
      "Epoch 008/015 | Batch 800/938 | Loss: 0.0470\n",
      "Epoch: 008/015 training accuracy: 99.25%\n",
      "Time elapsed: 1.01 min\n",
      "Epoch 009/015 | Batch 000/938 | Loss: 0.0024\n",
      "Epoch 009/015 | Batch 400/938 | Loss: 0.0083\n",
      "Epoch 009/015 | Batch 800/938 | Loss: 0.0625\n",
      "Epoch: 009/015 training accuracy: 99.11%\n",
      "Time elapsed: 1.15 min\n",
      "Epoch 010/015 | Batch 000/938 | Loss: 0.0015\n",
      "Epoch 010/015 | Batch 400/938 | Loss: 0.0013\n",
      "Epoch 010/015 | Batch 800/938 | Loss: 0.0050\n",
      "Epoch: 010/015 training accuracy: 99.42%\n",
      "Time elapsed: 1.29 min\n",
      "Epoch 011/015 | Batch 000/938 | Loss: 0.0009\n",
      "Epoch 011/015 | Batch 400/938 | Loss: 0.0036\n",
      "Epoch 011/015 | Batch 800/938 | Loss: 0.0413\n",
      "Epoch: 011/015 training accuracy: 99.08%\n",
      "Time elapsed: 1.42 min\n",
      "Epoch 012/015 | Batch 000/938 | Loss: 0.0107\n",
      "Epoch 012/015 | Batch 400/938 | Loss: 0.0107\n",
      "Epoch 012/015 | Batch 800/938 | Loss: 0.0175\n",
      "Epoch: 012/015 training accuracy: 99.20%\n",
      "Time elapsed: 1.55 min\n",
      "Epoch 013/015 | Batch 000/938 | Loss: 0.0047\n",
      "Epoch 013/015 | Batch 400/938 | Loss: 0.0051\n",
      "Epoch 013/015 | Batch 800/938 | Loss: 0.0862\n",
      "Epoch: 013/015 training accuracy: 99.32%\n",
      "Time elapsed: 1.69 min\n",
      "Epoch 014/015 | Batch 000/938 | Loss: 0.0010\n",
      "Epoch 014/015 | Batch 400/938 | Loss: 0.2128\n",
      "Epoch 014/015 | Batch 800/938 | Loss: 0.0212\n",
      "Epoch: 014/015 training accuracy: 99.34%\n",
      "Time elapsed: 1.82 min\n",
      "Epoch 015/015 | Batch 000/938 | Loss: 0.0030\n",
      "Epoch 015/015 | Batch 400/938 | Loss: 0.0065\n",
      "Epoch 015/015 | Batch 800/938 | Loss: 0.0459\n",
      "Epoch: 015/015 training accuracy: 99.25%\n",
      "Time elapsed: 1.95 min\n",
      "Total Training TIme: 1.95 min\n",
      "Test accuracy DoRA finetune: 97.56%\n",
      "++++\n",
      "Test accuracy: 97.03%\n",
      "Test accuracy DoRA finetune: 97.56%\n"
     ]
    }
   ],
   "source": [
    "optimizer_dora=torch.optim.Adam(model_dora.parameters(), lr=learning_rate)\n",
    "train(num_epochs, model_dora, optimizer_dora, train_loader, DEVICE)\n",
    "print(f'Test accuracy DoRA finetune: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')\n",
    "\n",
    "print('++++')\n",
    "\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy DoRA finetune: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
